{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageOps \n",
    "import numpy as np \n",
    "import os \n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "conf = (conf.setMaster('local[*]')\n",
    "        .set('spark.executor.memory', '4G')\n",
    "        .set('spark.driver.memory', '16G')\n",
    "        .set('spark.driver.maxResultSize', '10G'))\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resizing the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imageResize(basename,imageName):\n",
    "    \"\"\"\n",
    "    resize image\n",
    "    basename : eg. /home/username/XYZFolder\n",
    "    image name : xyz.jpg\n",
    "    New folder in the working directory will be created with '_resized' as suffix\n",
    "    \"\"\"\n",
    "    new_width  = 128\n",
    "    new_height = 128\n",
    "    try:  \n",
    "        img = Image.open(basename+\"/\"+imageName) # image extension *.png,*.jpg\n",
    "        img = img.resize((new_width, new_height), Image.ANTIALIAS)\n",
    "        img.save(basename+'_resized/'+imageName)\n",
    "    except:\n",
    "        os.mkdir(basename+'_resized/')\n",
    "        img = Image.open(basename+\"/\"+imageName) # image extension *.png,*.jpg\n",
    "        img = img.resize((new_width, new_height), Image.ANTIALIAS)\n",
    "        img.save(basename+'_resized/'+imageName)\n",
    "\n",
    "def resizer(folderPath):\n",
    "    \"\"\"\n",
    "    to resize all files present in a folder\n",
    "    resizer('/home/username/XYZFolder')\n",
    "    \"\"\"\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(folderPath):\n",
    "        for fileName in files:\n",
    "#             try:\n",
    "                #  print os.path.join(subdir, file)\n",
    "                filepath = subdir + os.sep + fileName\n",
    "                #  print filepath\n",
    "                if filepath.endswith(\".jpg\" or \".jpeg\" or \".png\" or \".gif\"):\n",
    "                    imageResize(subdir,fileName)\n",
    "#             except:\n",
    "#                 print traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# resizer('wiki_crop/wiki_crop_new/')\n",
    "# # went to wiki_crop/wiki_crop_new/_resized/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering & Converting images to pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'wiki_crop/wiki_crop_new/_resized/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterimage(path):\n",
    "    my_sub_dir = glob.glob(path + '*.jpg')\n",
    "    for i in my_sub_dir:\n",
    "        if os.path.getsize(i) < 1000:\n",
    "            # print(path + str(i) + '/' + str(j))\n",
    "            os.remove(i)\n",
    "\n",
    "#filter images that are corrupted\n",
    "# filterimage(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image2(infilename) :\n",
    "    '''  \n",
    "    convert image file to pixels\n",
    "    load_image2('fileName')\n",
    "    '''\n",
    "    img = Image.open(infilename).convert('L')\n",
    "    data = np.array(img)\n",
    "    return data\n",
    "\n",
    "\n",
    "def ageAtPhoto(fileName):\n",
    "    '''\n",
    "    get age at time of photo\n",
    "    ageAtPhoto('full_path_to_file')\n",
    "    10049200_1891-09-16_1958.jpg\n",
    "    yob is 1891\n",
    "    dtpt is 1958\n",
    "    '''\n",
    "    basename = fileName.split('/')[-1].split('_')\n",
    "    birth = int(basename[1].split('-')[0])\n",
    "    today = int(basename[2].split('.')[0])\n",
    "    currAge = abs(today - birth)\n",
    "    return currAge\n",
    "\n",
    "\n",
    "\n",
    "def convertToNumpy(folder):\n",
    "    '''\n",
    "    get pixels and age for each image in a folder\n",
    "    x_values, y_values = convertToNumpy(fileNames)    \n",
    "    '''\n",
    "    pixels = []\n",
    "    ages = []\n",
    "    filename =[]\n",
    "    for fileName in folder:\n",
    "#         if fileName.endswith(\".jpg\" or \".jpeg\" or \".png\"):\n",
    "            age = ageAtPhoto(fileName)\n",
    "            if (age<100 and age>0):\n",
    "                img_px = np.ravel(load_image2(fileName))\n",
    "                pixels.append(img_px)\n",
    "                ages.append(age)\n",
    "                filename.append(fileName.split('/')[-1])\n",
    "    return pixels, ages,filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderName = 'wiki_crop/wiki_crop_new/_resized/' \n",
    "fileNames = glob.glob(folderName +'*.jpg')\n",
    "# only test 20 files for now\n",
    "NumberOfFileToTrained =70000\n",
    "\n",
    "x_values, y_values,filename = convertToNumpy(fileNames[:NumberOfFileToTrained])\n",
    "\n",
    "# print x_values\n",
    "# print y_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14833"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting the data on RDD and converting to DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_rdd = sc.parallelize(x_values).map(lambda x : x.tolist()).map(lambda x: [int(element) for element in x])\n",
    "# len(flat_rdd.take(5)[0])\n",
    "age_rdd = sc.parallelize(y_values).map(lambda x:int(x))\n",
    "# age_rdd.take(5)\n",
    "f_name = sc.parallelize(filename)\n",
    "combined = flat_rdd.zip(age_rdd).zip(f_name)\n",
    "# combined.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "combined.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+\n",
      "|            features|label|              f_name|\n",
      "+--------------------+-----+--------------------+\n",
      "|[67, 67, 67, 65, ...|   21|31843216_1990-06-...|\n",
      "|[139, 140, 142, 1...|   46|1000001952958_196...|\n",
      "|[0, 0, 0, 0, 0, 0...|   98|12872267_1910-10-...|\n",
      "|[224, 223, 216, 2...|   19|43481633_1995-11-...|\n",
      "|[220, 219, 218, 2...|   52|1000001671401_195...|\n",
      "|[3, 3, 3, 3, 3, 3...|   59|100000185210_1944...|\n",
      "|[21, 19, 18, 21, ...|   27|1000007243240_197...|\n",
      "|[196, 196, 196, 1...|   31|10000022821347_19...|\n",
      "|[35, 35, 35, 35, ...|   61|10000029563647_19...|\n",
      "|[23, 23, 23, 23, ...|   28|1000003156912_198...|\n",
      "|[61, 60, 60, 62, ...|   39|10000026256606_19...|\n",
      "|[83, 71, 57, 54, ...|   34|610477_1976-04-20...|\n",
      "|[123, 123, 123, 1...|   45|30948829_1963-03-...|\n",
      "|[204, 204, 204, 2...|   34|10000012277158_19...|\n",
      "|[0, 0, 0, 0, 0, 0...|   27|1000001924882_194...|\n",
      "|[252, 252, 252, 2...|   24|10000015230998_19...|\n",
      "|[195, 200, 205, 2...|   63|10000028738868_19...|\n",
      "|[138, 137, 142, 1...|   23|19187533_1960-09-...|\n",
      "|[103, 103, 102, 1...|   28|1000001462396_198...|\n",
      "|[70, 69, 70, 70, ...|   30|680054_1980-04-22...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create a DataFrame\n",
    "imageschema = StructType([\n",
    "   StructField(\"features\", ArrayType(elementType=IntegerType(),containsNull=False),True),\n",
    "   StructField(\"label\", IntegerType(),True),\n",
    "   StructField(\"f_name\", StringType(),True)\n",
    "])\n",
    "df = sqlContext.createDataFrame(combined.map(lambda x : Row(x[0][0][:],x[0][1],x[1])), imageschema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = false)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- f_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download df in json format\n",
    "# df.write.format('json').save('project/dataset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+\n",
      "|            features|label|              f_name|\n",
      "+--------------------+-----+--------------------+\n",
      "|[67, 67, 67, 65, ...|   21|31843216_1990-06-...|\n",
      "|[139, 140, 142, 1...|   46|1000001952958_196...|\n",
      "|[0, 0, 0, 0, 0, 0...|   98|12872267_1910-10-...|\n",
      "|[224, 223, 216, 2...|   19|43481633_1995-11-...|\n",
      "|[220, 219, 218, 2...|   52|1000001671401_195...|\n",
      "|[3, 3, 3, 3, 3, 3...|   59|100000185210_1944...|\n",
      "|[21, 19, 18, 21, ...|   27|1000007243240_197...|\n",
      "|[196, 196, 196, 1...|   31|10000022821347_19...|\n",
      "|[35, 35, 35, 35, ...|   61|10000029563647_19...|\n",
      "|[23, 23, 23, 23, ...|   28|1000003156912_198...|\n",
      "|[61, 60, 60, 62, ...|   39|10000026256606_19...|\n",
      "|[83, 71, 57, 54, ...|   34|610477_1976-04-20...|\n",
      "|[123, 123, 123, 1...|   45|30948829_1963-03-...|\n",
      "|[204, 204, 204, 2...|   34|10000012277158_19...|\n",
      "|[0, 0, 0, 0, 0, 0...|   27|1000001924882_194...|\n",
      "|[252, 252, 252, 2...|   24|10000015230998_19...|\n",
      "|[195, 200, 205, 2...|   63|10000028738868_19...|\n",
      "|[138, 137, 142, 1...|   23|19187533_1960-09-...|\n",
      "|[103, 103, 102, 1...|   28|1000001462396_198...|\n",
      "|[70, 69, 70, 70, ...|   30|680054_1980-04-22...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write df to MongoDB\n",
    "df.write.format(\"com.mongodb.spark.sql.DefaultSource\").mode(\"append\").save()\n",
    "\n",
    "# Read df from MongoDB\n",
    "df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the df to train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "df = df.select(list_to_vector_udf(df[\"features\"]).alias(\"features\"),'label','f_name')\n",
    "\n",
    "dataset = df.randomSplit([0.8, 0.2])\n",
    "train = dataset[0].cache()\n",
    "test = dataset[1].cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+\n",
      "|            features|label|              f_name|\n",
      "+--------------------+-----+--------------------+\n",
      "|[0.0,0.0,0.0,0.0,...|    8|29307820_1984-05-...|\n",
      "|[0.0,0.0,0.0,0.0,...|   11|12321426_1951-04-...|\n",
      "|[0.0,0.0,0.0,0.0,...|   15|2729047_1981-01-2...|\n",
      "|[0.0,0.0,0.0,0.0,...|   19|10000046942333_19...|\n",
      "|[0.0,0.0,0.0,0.0,...|   20|10000038904783_19...|\n",
      "|[0.0,0.0,0.0,0.0,...|   20|37068594_1993-06-...|\n",
      "|[0.0,0.0,0.0,0.0,...|   21|10000041491567_19...|\n",
      "|[0.0,0.0,0.0,0.0,...|   21|17474041_1951-10-...|\n",
      "|[0.0,0.0,0.0,0.0,...|   21|43684651_1963-04-...|\n",
      "|[0.0,0.0,0.0,0.0,...|   22|10000012249351_19...|\n",
      "|[0.0,0.0,0.0,0.0,...|   22|10000033328513_19...|\n",
      "|[0.0,0.0,0.0,0.0,...|   22|10000038381611_19...|\n",
      "|[0.0,0.0,0.0,0.0,...|   22|2587518_1983-01-3...|\n",
      "|[0.0,0.0,0.0,0.0,...|   22|42356895_1992-10-...|\n",
      "|[0.0,0.0,0.0,0.0,...|   23|10000029763303_19...|\n",
      "|[0.0,0.0,0.0,0.0,...|   23|10000044065591_19...|\n",
      "|[0.0,0.0,0.0,0.0,...|   23|27045513_1987-03-...|\n",
      "|[0.0,0.0,0.0,0.0,...|   23|29763303_1988-01-...|\n",
      "|[0.0,0.0,0.0,0.0,...|   24|10000027662227_19...|\n",
      "|[0.0,0.0,0.0,0.0,...|   24|1000005592045_198...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+-----+--------------------+\n",
      "|            features|label|              f_name|\n",
      "+--------------------+-----+--------------------+\n",
      "|[0.0,0.0,0.0,0.0,...|   21|19215308_1986-05-...|\n",
      "|[0.0,0.0,0.0,0.0,...|   22|10000046408247_19...|\n",
      "|[0.0,0.0,0.0,0.0,...|   25|10000019521511_19...|\n",
      "|[0.0,0.0,0.0,0.0,...|   26|1000003749782_197...|\n",
      "|[0.0,0.0,0.0,0.0,...|   47|971296_1953-02-03...|\n",
      "|[1.0,2.0,3.0,4.0,...|   35|2986476_1965-09-2...|\n",
      "|[3.0,3.0,3.0,3.0,...|   36|6241134_1975-04-1...|\n",
      "|[3.0,3.0,3.0,3.0,...|   32|10000030408996_19...|\n",
      "|[8.0,8.0,8.0,8.0,...|   18|727925_1982-07-25...|\n",
      "|[8.0,46.0,78.0,12...|   54|14854413_1932-04-...|\n",
      "|[12.0,13.0,13.0,1...|   47|8281349_1942-10-0...|\n",
      "|[13.0,13.0,13.0,1...|    4|1000002521579_196...|\n",
      "|[14.0,12.0,18.0,3...|   31|1000008246146_197...|\n",
      "|[15.0,38.0,28.0,1...|   43|1000002620563_195...|\n",
      "|[16.0,16.0,16.0,1...|   33|1000001799831_197...|\n",
      "|[17.0,16.0,16.0,1...|   23|31302282_1987-11-...|\n",
      "|[21.0,20.0,21.0,2...|   40|8131982_1971-09-1...|\n",
      "|[23.0,22.0,21.0,2...|   44|5071413_1971-10-2...|\n",
      "|[29.0,29.0,27.0,2...|   44|100000718236_1967...|\n",
      "|[32.0,32.0,32.0,3...|   24|2647137_1924-10-1...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show()\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(maxIter=10, fitIntercept=True)\n",
    "lrmodel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(maxIter=10, fitIntercept=True)\n",
    "lrmodel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|            features|label|              f_name|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|[0.0,0.0,0.0,0.0,...|   21|19215308_1986-05-...|[-4.0758157443478...|[8.32752880314212...|      26.0|\n",
      "|[0.0,0.0,0.0,0.0,...|   22|10000046408247_19...|[-4.0758157443478...|[8.32752880314212...|      26.0|\n",
      "|[0.0,0.0,0.0,0.0,...|   25|10000019521511_19...|[-4.0758157443478...|[8.32752880314212...|      26.0|\n",
      "|[0.0,0.0,0.0,0.0,...|   26|1000003749782_197...|[-4.0758157443478...|[8.32752880314212...|      26.0|\n",
      "|[0.0,0.0,0.0,0.0,...|   47|971296_1953-02-03...|[-4.0758157443478...|[8.32752880314212...|      26.0|\n",
      "|[1.0,2.0,3.0,4.0,...|   35|2986476_1965-09-2...|[-4.5217781994925...|[4.17183340552383...|      29.0|\n",
      "|[3.0,3.0,3.0,3.0,...|   36|6241134_1975-04-1...|[-4.4949556592175...|[3.82023711901732...|      32.0|\n",
      "|[3.0,3.0,3.0,3.0,...|   32|10000030408996_19...|[-4.2448008749397...|[5.74375762935088...|      24.0|\n",
      "|[8.0,8.0,8.0,8.0,...|   18|727925_1982-07-25...|[-4.3771363420461...|[5.29624644266115...|      59.0|\n",
      "|[8.0,46.0,78.0,12...|   54|14854413_1932-04-...|[-4.4946333633698...|[4.78593807797864...|      29.0|\n",
      "|[12.0,13.0,13.0,1...|   47|8281349_1942-10-0...|[-4.3702743253167...|[4.82393351170126...|      29.0|\n",
      "|[13.0,13.0,13.0,1...|    4|1000002521579_196...|[-4.4689350866601...|[2.17947846284442...|      24.0|\n",
      "|[14.0,12.0,18.0,3...|   31|1000008246146_197...|[-4.5448482084401...|[4.62204026106418...|      26.0|\n",
      "|[15.0,38.0,28.0,1...|   43|1000002620563_195...|[-4.4670744598916...|[5.05517181044373...|      47.0|\n",
      "|[16.0,16.0,16.0,1...|   33|1000001799831_197...|[-4.2344749270203...|[6.50075602770035...|      26.0|\n",
      "|[17.0,16.0,16.0,1...|   23|31302282_1987-11-...|[-4.3124284118318...|[5.63869758980551...|      32.0|\n",
      "|[21.0,20.0,21.0,2...|   40|8131982_1971-09-1...|[-4.3623578232362...|[4.70712570098060...|      31.0|\n",
      "|[23.0,22.0,21.0,2...|   44|5071413_1971-10-2...|[-4.3052651202191...|[5.29148233587549...|      27.0|\n",
      "|[29.0,29.0,27.0,2...|   44|100000718236_1967...|[-4.4790415048727...|[4.84901752802284...|      59.0|\n",
      "|[32.0,32.0,32.0,3...|   24|2647137_1924-10-1...|[-4.3324875253067...|[5.23771254925686...|      31.0|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "validpredict = lrmodel.transform(test)\n",
    "validpredict.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Testing on Individual Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'test_files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderName = 'test_files/' \n",
    "fileNames = glob.glob(folderName +'*.png')\n",
    "# only testing 4 files for now\n",
    "\n",
    "x_values, y_values,filename = convertToNumpy(fileNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_rdd = sc.parallelize(x_values).map(lambda x : x.tolist()).map(lambda x: [int(element) for element in x])\n",
    "age_rdd = sc.parallelize(y_values).map(lambda x:int(x))\n",
    "f_name = sc.parallelize(filename)\n",
    "combined = flat_rdd.zip(age_rdd).zip(f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+\n",
      "|            features|label|              f_name|\n",
      "+--------------------+-----+--------------------+\n",
      "|[118, 135, 133, 1...|   30|666666_1986-10-24...|\n",
      "|[255, 255, 255, 2...|   29|101010_1987-06-24...|\n",
      "|[22, 21, 21, 21, ...|   32|666777_1986-10-24...|\n",
      "|[79, 108, 105, 10...|   31|101011_1987-06-24...|\n",
      "+--------------------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create a DataFrame\n",
    "imageschema = StructType([\n",
    "   StructField(\"features\", ArrayType(elementType=IntegerType(),containsNull=False),True),\n",
    "   StructField(\"label\", IntegerType(),True),\n",
    "   StructField(\"f_name\", StringType(),True)\n",
    "])\n",
    "df = sqlContext.createDataFrame(combined.map(lambda x : Row(x[0][0][:],x[0][1],x[1])), imageschema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "df_test = df.select(list_to_vector_udf(df[\"features\"]).alias(\"features\"),'label','f_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|            features|label|              f_name|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|[118.0,135.0,133....|   30|666666_1986-10-24...|[-4.4491482230859...|[6.29815527043899...|      43.0|\n",
      "|[255.0,255.0,255....|   29|101010_1987-06-24...|[-4.7206409159015...|[3.82764225278437...|      21.0|\n",
      "|[22.0,21.0,21.0,2...|   32|666777_1986-10-24...|[-4.3567617354430...|[6.67115644994620...|      43.0|\n",
      "|[79.0,108.0,105.0...|   31|101011_1987-06-24...|[-4.4738548639421...|[3.60126166168097...|      25.0|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "validpredict = lrmodel.transform(df_test)\n",
    "validpredict.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
